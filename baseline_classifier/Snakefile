# This section build a neural net with motifs as filters
# And train a logistic classifier on the top of it
# Instead of training directly, it writes a training scripts that is ready to be submitted to cluster

rule assemble:
    input:
        motif = lambda wildcards: config[wildcards.motif_name]['motifs'],
        xtrain = lambda wildcards: config[wildcards.motif_name]['data']['train']['x'],
        xvalid = lambda wildcards: config[wildcards.motif_name]['data']['valid']['x'],
        ytrain = lambda wildcards: config[wildcards.motif_name]['data']['train']['y'],
        yvalid = lambda wildcards: config[wildcards.motif_name]['data']['valid']['y'],
    params:
        llr = lambda wildcards: config[wildcards.motif_name]['threshold'],
        background = lambda wildcards: config[wildcards.motif_name]['background']
    output:
        model = 'prototype/{motif_name}.init.hdf5',
        feature_train = 'data/{motif_name}.train.feature.hdf5',
        feature_valid = 'data/{motif_name}.valid.feature.hdf5'
    shell:
        '''python -u scripts/assemble.py --motif {input.motif} --xtrain {input.xtrain} --xvalid {input.xvalid}\
        --threshold {params.llr} --output {output.model} --background {params.background} \
        --output_train {output.feature_train} --output_valid {output.feature_valid} \
        --ytrain {input.ytrain} --yvalid {input.yvalid}'''

rule train:
    input:
        train = 'data/{motif_name}.train.feature.hdf5',
        valid = 'data/{motif_name}.valid.feature.hdf5'
    params:
        outfolder = lambda wildcards: 'train/{motif_name}/{wildcards.group}/',
        niter = lambda wildcards: config[wildcards.motif_name]['niter'],
        l1 = lambda wildcards: config['Training'][wildcards.group]['l1'],
        l2 = lambda wildcards: config['Training'][wildcards.group]['l2']
    output:
        'sbatch/{motif_name}_{group}.train.sbatch'
    run:
        sbatch = '''
#!/bin/bash
#SBATCH --job-name={taskname}_{group}
#SBATCH --output={taskname}_{group}.out
#SBATCH --error={taskname}_{group}.err
#SBATCH --time=24:00:00
#SBATCH --partition=gpu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=50G
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
cd /project2/xinhe/yanyul
source setup.sh
source activate keras
module load cuda/7.5
cd repo/deep_variant/baseline_classifier
mkdir {outdir}
python scripts/train_logistic_on_cluster.py \
--train {train} \
--valid {valid} \
--outdir {outdir} \
--batch_size {batch_size} \
--epoch {niter} \
--l1 {l1} \
--l2 {l2}
'''.format(taskname=wildcards.motid_name, train=input.train, valid=input.valid, niter=params.niter, outdir=params.outfolder, l1=params.l1, l2=params.l2, group=wildcards.group)
        o = open(output[0], 'w')
        o.write(sbatch)
        o.close()
