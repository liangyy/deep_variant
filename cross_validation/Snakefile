# This module takes a data sets and the goal is to test the performance (in terms of linear separatibility) of certain sequence feature representation on the data set.
# K-fold cross validation is used to evaluate the performance empirically.
# Two linear classifiers are implemented: logistic regression & SVM
#
# CAUTION:
# 1. This module works with relatively small data sets. Large data set may take long time to run
# 2. For SVM, hyperparameter affects the performance as well. So, it should be tuned

rule all:
    input:
        'report/performance.{data}__{feature}__{classifier}__{k}fold.html'.format(data = config['data']['name'], feature = config['feature']['name'], k = config['fold'], classifier = config['classifier']['type'] + '-' + config['classifier']['param'])

rule seq2feature:
    input:
        seq = config['data']['path'],
        feature = config['feature']['path']
    output:
        'output/{data}__{feature}.feature.hdf5'
    shell:
        'python scripts/seq2feature.py --input {input.seq} --feature {input.feature} --out {output[0]}'

rule cv:
    input:
        label = config['data']['path'],
        feature = 'output/{data}__{feature}.feature.hdf5'
    params:
        classifier = config['classifier']['type'],
        param = config['classifier']['param'],
        prefix = 'output/{data}__{feature}__{classifier}__{k}fold'
    output:
        [ 'output/{{data}}__{{feature}}__{{classifier}}__{{k}}fold-{i}.predict.txt.gz'.format(i = i + 1) for i in range(config['fold']) ],
        [ 'output/{{data}}__{{feature}}__{{classifier}}__{{k}}fold-{i}.best.hdf5'.format(i = i + 1) for i in range(config['fold']) ]
    shell:
        '''
        python scripts/keras_cv.py \
            --x {input.feature} \
            --y {input.label} \
            --classifier_type {params.classifier} \
            --classifier_param {params.param} \
            --fold {wildcards.k} \
            --prefix {params.prefix}
        '''

rule performance_report_rmd:
    input:
        [ 'output/{{data}}__{{feature}}__{{classifier}}__{{k}}fold-{i}.predict.txt.gz'.format(i = i + 1) for i in range(config['fold']) ]
    params:
        prefix = 'output/{data}__{feature}__{classifier}__{k}fold'
    output:
        'report/performance.{data}__{feature}__{classifier}__{k}fold.Rmd'
    run:
        '''---
title: "Cross validation -- Performance on data = {data}; feature = {feature}; classifier = {classifier}; {k}fold"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 3
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

```{{r setup}}
knitr::opts_knit$set(root.dir = '../../')
library(precrec)
library(pander)
library(dplyr)
```

# Load data

```{{r}}
fold <- {k}
prefix <- '{prefix}'
scores <- list()
truth <- list()
for(i in 1 : fold) {{
  temp <- read.table(paste0(prefix, i, 'predict.txt.gz'), header = T)
  scores[[paste('round', i)]] <- temp$y_pred
  truth[[paste('round', i)]] <- temp$y_true
}}
msmdat <- mmdata(scores, truth, dsids = 1 : fold)
mscurves <- evalmod(msmdat)
plot(mscurves)
df <- auc(mscurves)
df.auc <- df %>%
  group_by(curvetypes) %>%
  summarise(AUC = mean(aucs), SE = sd(aucs))
pander(df.auc)
```
'''.format(data = wildcards.data, feature = wildcards.feature, classifier = wildcards.classifier, k = wildcards.k, prefix = params.prefix)

rule performance_report_html:
    input:
        rmd = 'report/performance.{data}__{feature}__{classifier}__{k}fold.Rmd',
        data = [ 'output/{{data}}__{{feature}}__{{classifier}}__{{k}}fold-{i}.predict.txt.gz'.format(i = i + 1) for i in range(config['fold']) ]
    output:
        'report/performance.{data}__{feature}__{classifier}__{k}fold.html'
    shell:
        '''Rscript -e "rmarkdown::render('./{input.rmd}')"'''
