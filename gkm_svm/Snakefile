# This section build the pipeline to make prediction using gkm-SVM
# desribed in [this](https://github.com/Dongwon-Lee/lsgkm) repository

# The pipeline contains two part:
# 1. training: hdf5 -> fasta; training
# 2. prediction: hdf5 -> fasta; predicting
# due to the capability of the software, we may randomly select a
# subset of sequences to make prediction on

rule sampling_from_training_set:
    input:
        x = lambda wildcards: config['training_data'][wildcards.trdata_name]['x'],
        y = lambda wildcards: config['training_data'][wildcards.trdata_name]['y']
    params:
        n = config['repeat'],
        prefix = lambda wildcards: 'data/{trdata_name}_{label}'.format(trdata_name=wildcards.trdata_name, label = wildcards.label),
        size = config['size'],
        seed = config['seed'],
        label = lambda wildcards: config['label'][wildcards.label]
    output:
        pos = temp([ 'data/{{trdata_name}}_{{label}}.{id}.pos.hdf5'.format(id=i+1) for i in range(config['repeat']) ]),
        neg = temp([ 'data/{{trdata_name}}_{{label}}.{id}.neg.hdf5'.format(id=i+1) for i in range(config['repeat']) ])
    shell:
        '''python scripts/split_training_set.py --x {input.x} \
        --y {input.y} --nsubset {params.n} \
        --prefix {params.prefix} --size {params.size} \
        --seed {params.seed} --label {params.label} '''

rule hdf5_to_fasta_valid:
    input:
        x = lambda wildcards: config['validation_data'][wildcards.vldata_name]['x']
        # y = lambda wildcards: config['validation_data'][wildcards.trdata_name]['y']
    output:
        'data/{vldata_name}_{label}.all.fa'
        # neg = 'data/{trdata_name}_{label}.neg.fa'
    params:
        config['window_size']
    shell:
        '''python scripts/hdf5_to_fasta.py --split 1 --window {params[0]} \
        --data {input.x} --out {output[0]}'''

rule hdf5_to_fasta_train:
    input:
        'data/{trdata_name}_{label}.{id}.{state}.hdf5'
    output:
        'data/{trdata_name}_{label}.{id}.{state}.fa'
    params:
        config['window_size']
    shell:
        '''python scripts/hdf5_to_fasta.py --split 1 --window {params[0]} \
        --data {input[0]} --out {output[0]}'''

rule training:
    input:
        pos = 'data/{trdata_name}_{label}.{id}.pos.fa',
        neg = 'data/{trdata_name}_{label}.{id}.neg.fa'
    output:
        'sbatch/{trdata_name}_{label}.{id}_train.sbatch'
    # threads:
    #     config['thread']
    log:
       'logs/train_{trdata_name}_{label}.{id}.log' # .format(trdata_name=wildcards.trdata_name, label=wildcards.label, id=wildcards.id)
    params:
        prefix = lambda wildcards: 'model/{trdata_name}_{label}.{id}'.format(trdata_name=wildcards.trdata_name, label=wildcards.label, id=wildcards.id),
        cache = config['cache'],
        name = lambda wildcards: '{trdata_name}_{label}.{id}'.format(trdata_name=wildcards.trdata_name, label=wildcards.label, id=wildcards.id),
        # out = lambda wildcards: 'model/{trdata_name}_{label}.{id}.model.txt'.format(trdata_name=wildcards.trdata_name, label=wildcards.label, id=wildcards.id)
    # shell:
    #     '''gkmtrain -m {params.cache} -T {threads} {input.pos} {input.neg} {params.prefix} > {log}'''
    run:
        sbatch = '''#!/bin/bash
#SBATCH --job-name={name}_train
#SBATCH --output={name}_train.out
#SBATCH --error={name}_train.err
#SBATCH --time=24:00:00
#SBATCH --partition=broadwl
#SBATCH --mem-per-cpu=50G
#SBATCH --nodes=1

cd /project2/xinhe/yanyul
source setup.sh
source activate deepvarpred_test
cd repo/deep_variant/allelic_imbalance
gkmtrain -m {cache} {pos} {neg} {prefix} > {log}
'''.format(cache=params.cache, pos=input.pos, neg=input.neg, prefix=params.prefix, log=log, name=params.name)
        o = open(output[0], 'w')
        o.write(sbatch)
        o.close()

rule validation_raw_output:
    input:
        model = 'model/{trdata_name}_{label}.{id}.model.txt',
        data = 'data/{vldata_name}_{label}.all.fa'
    output:
        temp('result/{trdata_name}_{label}.{id}.{vldata_name}.txt')
    threads:
        config['thread']
    log:
        'logs/valid_{trdata_name}_{label}.{id}.{vldata_name}.log'
    shell:
        '''gkmpredict -T {threads} {input.data} {input.model} {output[0]} > {log}; cat {output[0]} | cut -f2 > temp ; mv temp {output[0]}'''

rule logit_raw_and_to_feather:
    input:
        pred = 'result/{trdata_name}_{label}.{id}.{vldata_name}.txt',
        y = lambda wildcards: config['validation_data'][wildcards.vldata_name]['y']
    params:
        label = lambda wildcards: config['label'][wildcards.label]
    output:
        'result/{trdata_name}_{label}.{id}.{vldata_name}.feather'
    shell:
        'python scripts/raw_to_feather.py --predict {input.pred} --truth {input.y} --label {params.label} --out {output[0]}'

def get_all_results(config):
    files = []
    trdata_name = next (iter (config['training_data'].keys()))
    vldata_name = next (iter (config['validation_data'].keys()))
    # print([trdata_name, vldata_name])
    for label in config['label']:
        for i in range(config['repeat']):
            files.append('result/{trdata_name}_{label}.{id}.{vldata_name}.feather'.format(trdata_name=trdata_name, vldata_name=vldata_name, label=label, id=i+1))
    return files

rule report_training_rmd:
    input:
        files = get_all_results(config)
    output:
        'report/{trdata_name}.training_report.Rmd'
    params:
        labels = '$$'.join(config['label']),
        repeats = '$$'.join([ str(i+1) for i in range(config['repeat']) ])
    run:
        rmd = '''---
title: "Deep brain - Training gkm-SVM"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 3
    theme: cosmo
    toc_float: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{{r setup}}
knitr::opts_knit$set(root.dir = '../')
```

# Import results

```{{r}}
library(feather)
library(ggplot2)
library(precrec)
labels <- '{labels}'
repeats <- '{repeats}'
trdata_name <- 'deepsea_train_small'
vldata_name <- 'deepsea_valid'
all_scores <- list()
all_labels <- list()
for(label in strsplit(labels, '$$', fixed = T)[[1]]){{
  scores <- list()
  for(i in strsplit(repeats, '$$', fixed = T)[[1]]){{
    filename <- paste0('result/', trdata_name, '_', label, '.', i, '.', vldata_name, '.feather')
    data <- read_feather(filename)
    scores[[i]] <- data$y.predict
  }}
  all_labels[[label]] <- data$y
  all_scores[[label]] <- scores
}}
```

# ROC/PR curves by label

```{{r, results='asis'}}
library(pander)
panderOptions('knitr.auto.asis', FALSE)
for (label in strsplit(labels, '$$', fixed = T)[[1]]){{
    cat('\n')
    cat('##', label, '\n')
    msmdat <- mmdata(all_scores[[label]], all_labels[[label]], modnames = names(all_scores[[label]]))
    mscurves <- evalmod(msmdat)
    plot(mscurves)
    cat('\n')
    pander(auc(mscurves))
    cat('\n')
}}
```
'''.format(labels=params.labels, repeats=params.repeats)
        o = open(output[0], 'w')
        o.write(rmd)
        o.close()
