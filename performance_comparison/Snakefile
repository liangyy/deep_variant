# This module takes the intermediate output of pr_roc_curve module (the curve feather file) do the bar plot to
# compare the performance of multiple classifiers under multiple data sets

rule calculate_auc:
    input:
        lambda wildcards: config[wildcards.task][wildcards.model]
    output:
        'result/{task}/auc_{model}.feather'
    shell:
        'python scripts/auc.py --curve {input[0]} --out {output[0]}'
rule summary_rmd:
    # input:
    #     [ 'result/auc_{model}.feather'.format(model=model) for model in config ]
    output:
        temp('report/{task}.auc_comparison.Rmd')
    params:
        lambda wildcards: '--'.join(config[wildcards.task]),
	    lambda wildcards: wildcards.task,
        config['by']
    run:
        rmd = '''---
title: "Training - Compare AUCs"
output:
    html_document:
        number_sections: true
        toc: true
        toc_depth: 3
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

```{{r setup}}
knitr::opts_knit$set(root.dir = '../')
```

```{{r}}
library(feather)
library(ggplot2)
model_list <- '{models}'
by <- '{by}'
aucs <- c()
for(i in strsplit(model_list, '--')[[1]]){{
    temp <- read_feather(paste0('result/{task}/auc_', i, '.feather'))
    if(by == 'model'){{
        temp$model <- i
        colnames(temp)[colnames(temp) == 'info'] <- 'sequence'
    }}else{{
        temp$sequence <- i
        colnames(temp)[colnames(temp) == 'info'] <- 'model'
    }}
    aucs <- rbind(aucs, temp)
}}
aucs$grp <- paste(aucs$model, aucs$sequence)
temp <- unique(aucs$info)
cols <- c('red', 'black')
names(cols) <- temp
ggplot(aucs) + geom_bar(aes(x = data, y = score, group = grp, fill = model), stat = 'identity', position = 'dodge') +
facet_wrap(~curve) + # scale_colour_manual(values = cols) +
# scale_fill_brewer(palette="OrRd") +
# labs(color = 'sequence source') +
ggtitle('AUCs')
```
'''.format(models=params[0], task=params[1], by=params[2])
        o = open(output[0], 'w')
        o.write(rmd)
        o.close()

rule summary_html:
    input:
        rmd = 'report/{task}.auc_comparison.Rmd',
        files = lambda wildcards: [ 'result/{task}/auc_{model}.feather'.format(model=model, task=wildcards.task) for model in config[wildcards.task] ]
    output:
        'report/{task}.auc_comparison.html'
    shell:
        '''Rscript -e "rmarkdown::render('./{input.rmd}')"'''
